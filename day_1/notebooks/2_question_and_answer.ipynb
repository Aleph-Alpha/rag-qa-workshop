{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "AA_TOKEN=os.getenv(\"AA_TOKEN\")\n",
    "NAMESPACE=os.getenv(\"AA_NAMESPACE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question & Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SingleChunkQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Sequence\n",
    "from intelligence_layer.core import (\n",
    "    Task,\n",
    "    TextHighlight,\n",
    "    CompleteInput,\n",
    "    TaskSpan,\n",
    "    LuminousControlModel,\n",
    "    NoOpTracer,\n",
    "    TextHighlightInput,\n",
    ")\n",
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class Input(BaseModel):\n",
    "    chunk: str\n",
    "    question: str\n",
    "\n",
    "\n",
    "class Output(BaseModel):\n",
    "    answer: str\n",
    "    highlights: Sequence[str]\n",
    "\n",
    "\n",
    "class SingleChunkQaTask(Task[Input, Output]):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: LuminousControlModel | None = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self._model = model or LuminousControlModel(\"luminous-nextgen-7b-control-384k\")\n",
    "\n",
    "    def do_run(self, input: Input, task_span: TaskSpan) -> Output:\n",
    "        instruction = f'Beantworte die Frage anhand des Textes. Wenn sich die Frage nicht mit dem Text beantworten lÃ¤sst, antworte \"Keine Antwort in den Quellen\".\\nQuellen: {input.chunk}\\nFrage: {input.question}'\n",
    "        prompt = self._model.to_instruct_prompt(instruction=instruction)\n",
    "        input = CompleteInput(prompt=prompt)\n",
    "        output = self._model.complete(input, task_span)\n",
    "\n",
    "        highlight_task = TextHighlight(model=self._model)\n",
    "        highlight_input = TextHighlightInput(\n",
    "            rich_prompt=prompt,\n",
    "            target=output.completion,\n",
    "        )\n",
    "        highlight_output = highlight_task.run(highlight_input, task_span)\n",
    "\n",
    "        print(prompt.items)\n",
    "        return Output(\n",
    "            answer=output.completion,\n",
    "            highlights=[\n",
    "                prompt.items[0].text[highlight.start : highlight.end]\n",
    "                for highlight in highlight_output.highlights\n",
    "                if highlight.score > 0\n",
    "            ],\n",
    "        )\n",
    "\n",
    "\n",
    "question = \"Wer mag Pizza?\"\n",
    "chunk = \"Tina mag nicht so gern Pizza. Mike aber mag Pizza sehr.\"\n",
    "\n",
    "task = SingleChunkQaTask()\n",
    "result = task.run(input=Input(chunk=chunk, question=question), tracer=NoOpTracer())\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk\n",
    "result.highlights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MultiChunkRetrieverQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_name = \"rag-collection\"\n",
    "index_name = \"rag-index\"\n",
    "number_of_chunks = 5\n",
    "query = \"Was ist nohtyp?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from intelligence_layer.connectors import (\n",
    "    DocumentIndexClient,\n",
    "    DocumentIndexRetriever,\n",
    ")\n",
    "from intelligence_layer.core import InMemoryTracer, LuminousControlModel\n",
    "from intelligence_layer.examples import MultipleChunkRetrieverQa, RetrieverBasedQaInput\n",
    "\n",
    "\n",
    "document_index = DocumentIndexClient(\n",
    "    token=AA_TOKEN,\n",
    ")\n",
    "\n",
    "document_index_retriever = DocumentIndexRetriever(\n",
    "    document_index=document_index,\n",
    "    index_name=index_name,\n",
    "    namespace=NAMESPACE,\n",
    "    collection=collection_name,\n",
    "    k=5,\n",
    "    threshold=0.5,\n",
    ")\n",
    "\n",
    "model = LuminousControlModel(\"luminous-nextgen-7b-control-384k\")\n",
    "retriever_qa = MultipleChunkRetrieverQa(\n",
    "    document_index_retriever, \n",
    "    insert_chunk_number=number_of_chunks,\n",
    ")\n",
    "\n",
    "input = RetrieverBasedQaInput(\n",
    "    question=query,\n",
    ")\n",
    "tracer = InMemoryTracer()\n",
    "\n",
    "output = retriever_qa.run(input, tracer)\n",
    "print(\"Answer: \", output.answer)\n",
    "print(\"-\"*50)\n",
    "print(\"Information:\")\n",
    "print(output.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
