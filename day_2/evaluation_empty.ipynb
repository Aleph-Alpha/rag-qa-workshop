{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import cast\n",
    "import os\n",
    "import urllib.parse\n",
    "\n",
    "import pandas as pd\n",
    "from datasets import DatasetDict, load_dataset\n",
    "from intelligence_layer.connectors import (\n",
    "    DocumentIndexClient,\n",
    "    DocumentPath,\n",
    "    CollectionPath,\n",
    "    DocumentContents,\n",
    " DocumentIndexRetriever\n",
    ")\n",
    "from intelligence_layer.evaluation import (\n",
    "    Dataset,\n",
    "    Example,\n",
    "    RepositoryNavigator,\n",
    "    RunOverview,\n",
    "    EvaluationOverview,\n",
    "    AggregationOverview,\n",
    "    run_lineages_to_pandas,\n",
    "    evaluation_lineages_to_pandas,\n",
    "    aggregation_overviews_to_pandas,\n",
    "    Evaluator,\n",
    "    Runner,\n",
    "    Aggregator,\n",
    ")\n",
    "from intelligence_layer.examples import (\n",
    "    RetrieverBasedQaInput,\n",
    "    MultipleChunkRetrieverQaOutput,\n",
    "    MultipleChunkRetrieverQa,\n",
    ")\n",
    "from intelligence_layer.core import (\n",
    "    Language,\n",
    "    ControlModel,\n",
    "    LuminousControlModel,\n",
    "    Llama3InstructModel,\n",
    "    Task,\n",
    "    TaskSpan\n",
    ")\n",
    "from pydantic import BaseModel, ConfigDict\n",
    "\n",
    "from rewe_workshop.repositories import init_repos\n",
    "from rewe_workshop.evaluation import (\n",
    "    RetrieverQaEvaluationLogic,\n",
    "    RetrieverQaAggregationLogic,\n",
    "    RetrieverQaEvaluation,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Dataset\n",
    "\n",
    "1. Load the following HF dataset: `deepset/germanquad` (only use a sample, e.g. 10).\n",
    "2. Insert the context into the document index.\n",
    "3. Save the dataset in a `FileDatasetRepository`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store Contexts in Document Index for Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store Questions in DatasetRepository for Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Step\n",
    "1. Create a `FileRunRepository` and a `Runner` and run the `MultipleChunkRetrieverQA` task for our dataset.\n",
    "2. Create a `RepositoryNavigator` and retrieve the lineages for the run.\n",
    "3. Convert the lineages to a pandas DataFrame with the `run_lineages_to_pandas` method.\n",
    "4. Restructure the code, so that you can specify multiple configurations for the task. Then each configuration should create its own run. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Step\n",
    "\n",
    "1. Write a custom `EvaluationLogic` and use it on your runs with an `Evaluator`. The custom `EvaluationLogic` does not need to do anything sensible as a first step.\n",
    "2. Write a new task `WorldKnowledgeGrader` that detects whether a given text contains information that is not found in a reference text. Use the classes below as a template.\n",
    "3. Adapt your custom `EvaluationLogic` to use the `WorldKnowledgeGrader`.\n",
    "4. Retrieve the `EvaluationLineage`s with the `RepositoryNavigator` and convert them to a Dataframe with the `evaluation_lineages_to_pandas` function.   \n",
    "**Hint**: To make the dataframe output more readable, you can use the `expand_pydantic_column` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WorldKnowledgeGradingInput(BaseModel):\n",
    "    reference_text: str\n",
    "    compare_text: str\n",
    "\n",
    "\n",
    "class WorldKnowledgeGradingOutput(BaseModel):\n",
    "    reasoning: str\n",
    "    contains_world_knowledge: bool\n",
    "\n",
    "class WorldKnowledgeGrader(\n",
    "    Task[WorldKnowledgeGradingInput, WorldKnowledgeGradingOutput]\n",
    "):\n",
    "    def __init__(self, model: ControlModel):\n",
    "        super().__init__()\n",
    "        self._model = model\n",
    "\n",
    "    def do_run(\n",
    "        self, input: WorldKnowledgeGradingInput, task_span: TaskSpan\n",
    "    ) -> WorldKnowledgeGradingOutput:\n",
    "        return WorldKnowledgeGradingOutput(\n",
    "            reasoning=\"\",\n",
    "            contains_world_knowledge=False,\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_pydantic_column(df: pd.DataFrame, column: str):\n",
    "    normalized = pd.json_normalize(df[column].map(lambda x: x.model_dump())).add_prefix(\n",
    "        column + \".\"\n",
    "    )\n",
    "\n",
    "    normalized.index = df.index\n",
    "    return pd.concat(\n",
    "        [\n",
    "            df,\n",
    "            normalized,\n",
    "        ],\n",
    "        axis=1,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Optional) Manually edit the run example output to add hallucination."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregation Step\n",
    "1. Create a `FileAggregationRepository` and an `Aggregator` and aggregate each evaluation from the previous step.\n",
    "2. Convert the resulting `AggregationOverview`s to pandas with the `aggregation_overviews_to_pandas` function and look at the final results.\n",
    "3. (Optional) Edit a run manually to produce a hallucinated answer to check whether the evaluation actually works.\n",
    "\n",
    "**Hint**: You can use the following code snippet to show the run description in the dataframe.\n",
    "\n",
    "```python\n",
    "aggregation_df[\"description\"] = aggregation_df[\"evaluation_overviews\"].map(\n",
    "    lambda evaluation_overviews: evaluation_overviews[0][\"run_overviews\"][0][\n",
    "        \"description\"\n",
    "    ]\n",
    ")\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
